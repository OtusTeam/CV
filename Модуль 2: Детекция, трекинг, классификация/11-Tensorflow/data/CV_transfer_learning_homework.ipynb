{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV-transfer_learning_homework.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZXRqftp-32"
      },
      "source": [
        "# Домашнее задание по transfer learning\n",
        "\n",
        "* Тема: выполнить transfer learning + fine-tuning для модели на собранном датасете. Примеры датасетов: киты и акулы, снятых под водой; экзотические птицы; фрукты и овощи; и так далее.\n",
        "* Работа ведется на tensorflow. За основу можно взять готовые материалы или ноутбуки: [tf](https://colab.research.google.com/drive/18N3JWFS7v2MB_9owoLpXZE_NS8orGo2h?usp=sharing), [keras.io](https://keras.io/guides/transfer_learning/), (если ссылки не работают, напишитe в slack).\n",
        "* Результатом работы должен быть обученный классификатор:\n",
        "  * Сама модель и графики ее обучения\n",
        "  * Пример работы на нескольких изображениях\n",
        "  * Ноутбук с кодом обучения\n",
        "\n",
        "# Более подробные инструкции\n",
        "* Выполнять д/з можно в ноутбуках, скриптах .py, colab - где удобнее.\n",
        "* Для сборки датасета используйте [fatkun](https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf?hl=en). Работает, к сожалению, только в Google Chrome. Можно использовать и другие инструменты (js/python скрипты), но времени потратите больше, скорее всего.\n",
        "* Для начала сборки датасета попробуйте два запроса: whale underwater, shark underwater. Начните с нескольких сотен изображений (подготовка займет ~40 минут), обучите сеть на них. (*Если у вас мало времени/другие сложности со сбором данных, можете взять готовый датасет, инструкция в конце ноутбука. Но советую попробовать собрать свой*)\n",
        "* Не нужно чрезмерных усилий, хватит ~100-200 картинок на каждый класс для старта. Не забывайте про базовую аугментацию.\n",
        "* Не забудьте сделать train, validation, test - сплиты! Тестовый сплит понадобится для честной model selection (напоминаю, что early stopping - это тоже техника model selection).\n",
        "* Задание:\n",
        "  * Подготовьте данные. Поскольку дальше будут использоваться параметры ImageNet, рекомендую приводить картинки к формату (224, 224) и использовать такой же размер входного слоя сети.\n",
        "  * Постройте t-SNE представление векторов, полученных feature extractor'ом. (можно поэкспериментировать с t-SNE, в частности, попробовать 3D-графики).\n",
        "  * Поэкспериментируйте с разным количеством добавленных слоев классификатора и количеством нейронов в них. *Будет ли модель обучаться, и если да, то в каких условиях*, если добавить один полносвязный слой классификатора непосредственно к feature extractor'у? \n",
        "  * Обучите модель transfer learning, взяв за основу любую модель, обученную на ImageNet. Если у вас два класса, постарайтесь получить accuracy 0.75+ для сбалансированной тестовой выборки :) Можно больше.\n",
        "  * После того, как модель будет обучена, выполните fine-tuning, разморозив основную часть весов модели. Постройте предствление t-SNE векторных представлений после fine-tuning'a и сравните с тем, которое было до него. Fine-tuning можно выполнять как с метками, так и при помощи triplet loss'a.\n",
        "  * Также добавьте loss/accuracy графики для обучающих и валидационных данных.Можно сделать это в tensorboard.\n",
        "  * *Дополнительно*: подумайте, как сочетание подхода с поиском векторных представлений и аугментаию можно использовать для обучения на данных с частичной или отсутсвующей разметкой. Интересно обсудить ваши идеи.\n",
        "* Пришлите ссылку на ноутбук с кодом / скрипты как результаты работы.\n",
        "* Если есть вопросы - пишите в slack.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efIo90cbeO9S"
      },
      "source": [
        "# Код, который может понадобиться"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moyU0CLytbYj"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "from numpy.linalg import norm\n",
        "import tensorflow as tf\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def make_tsne_plot(vectors, labels):\n",
        "  pca_dimension = 100\n",
        "  if vectors.shape[1] <= pca_dimension * 2:\n",
        "    pca_vectors = vectors\n",
        "  else:\n",
        "    pca = PCA(pca_dimension)\n",
        "    pca.fit(vectors)\n",
        "    pca_vectors = pca.transform(vectors)\n",
        "\n",
        "  tsne_results = TSNE(n_components=2,\n",
        "                      verbose=1,\n",
        "                      metric='euclidean').fit_transform(pca_vectors) \n",
        "\n",
        "  cmap = plt.cm.get_cmap('coolwarm')\n",
        "  plt.figure(figsize=(10, 8), dpi=80)\n",
        "  scatter = plt.scatter(tsne_results[:,0],\n",
        "                        tsne_results[:,1], \n",
        "                        c=labels, \n",
        "                        cmap=cmap)\n",
        "  plt.colorbar(scatter)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def make_extractor(img_shape, model=None, \n",
        "                   dropout_rate=0.25, trainable=False):\n",
        "  if model is None:\n",
        "    extractor = keras.applications.InceptionResNetV2(\n",
        "        include_top=False, weights=\"imagenet\")\n",
        "  else:\n",
        "    extractor = model(include_top=False, weights=\"imagenet\")\n",
        "  for layer in extractor.layers:\n",
        "    layer.trainable = trainable\n",
        "\n",
        "  gap = keras.layers.GlobalAveragePooling2D()\n",
        "  dropout = keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  input = keras.layers.Input(img_shape)\n",
        "  x = extractor(input)\n",
        "  x = dropout(x)\n",
        "  x = gap(x)\n",
        "\n",
        "  return keras.Model(input, x)\n",
        "\n",
        "\n",
        "def make_triple_loss_model(img_shape, model=None, embedding_shape=32, \n",
        "                          units=64, dropout_rate=0.25, trainable=False): \n",
        "  \"\"\"\n",
        "  Создаем модель. По умолчанию - InceptionResNetV2.\n",
        "  Если передан конструктор model, используется он (должен поддерживать \n",
        "  интерфейс keras.applications).\n",
        "  \n",
        "  \"\"\"\n",
        "  extractor = make_extractor(img_shape, model, \n",
        "                             dropout_rate=dropout_rate, \n",
        "                             trainable=trainable)\n",
        "\n",
        "  dense_1 = keras.layers.Dense(units, activation='relu')\n",
        "  dense_2 = keras.layers.Dense(units, activation='relu')\n",
        "  embedding_layer = keras.layers.Dense(embedding_shape, activation='linear')\n",
        "  dropout = keras.layers.Dropout(0.25)\n",
        "\n",
        "  images = keras.layers.Input(img_shape, name='input_shape')\n",
        "  labels = keras.layers.Input((1,), name='input_label')\n",
        "\n",
        "  x = extractor(images)\n",
        "  x = dropout(x)\n",
        "  x = dense_1(x)\n",
        "  x = dropout(x)\n",
        "  x = dense_2(x)\n",
        "  embeddings = embedding_layer(x)\n",
        "\n",
        "  return keras.Model(inputs=images, outputs=embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujlnLvCZe0_o"
      },
      "source": [
        "## Обратите внимание!\n",
        "\n",
        "`ImageDataGenerator` может сам разделять данные из одной директории на train и val."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia7x_807eztu"
      },
      "source": [
        "\n",
        "dataget = keras.preprocessing.image.ImageDataGenerator(\n",
        "    zca_epsilon=1e-06,\n",
        "    rotation_range=0.25,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    channel_shift_range=0.0,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    rescale=1/255.,\n",
        "    validation_split=0.20  # <- Если выставлен этот параметр\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RD4GFP4fF06"
      },
      "source": [
        "Тогда при создании генераторов из `datatagen` вам будет нужно указать название сплита:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypBxUdbufUrL"
      },
      "source": [
        "dataset_path = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLgzc_1QfAJ2"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "IMG_SHAPE = (224, 224, 3)\n",
        "train_generator = dataget.flow_from_directory(\n",
        "    dataset_path, target_size=IMG_SHAPE[:-1],\n",
        "    batch_size=BATCH_SIZE, subset=\"training\")\n",
        "val_generator = dataget.flow_from_directory(\n",
        "    dataset_path, target_size=IMG_SHAPE[:-1],\n",
        "    batch_size=BATCH_SIZE, subset=\"validation\")\n",
        "training_steps = int(np.ceil(train_generator.n // BATCH_SIZE))\n",
        "val_steps = int(np.ceil(val_generator.n // BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9ICCZpuf1ss"
      },
      "source": [
        "# Если вам нужен готовый датасет, можно попробовать:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU07s_R8fTi_"
      },
      "source": [
        "# thx https://gist.github.com/frogermcs/ed9fc359941efe54cc80d5b15f87bf77\n",
        "import optparse\n",
        "\n",
        "data_root = tf.keras.utils.get_file(\n",
        "  'flower_photos',\n",
        "  'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "   untar=True)\n",
        "\n",
        "dataset_path = os.path.join(os.path.expanduser('~'), \n",
        "                            '.keras/datasets/flower_photos/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pRwFnLrgEgj"
      },
      "source": [
        "dataset_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5JF0yksgFgg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}