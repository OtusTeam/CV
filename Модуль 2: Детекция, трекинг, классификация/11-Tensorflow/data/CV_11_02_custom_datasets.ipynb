{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV-11-02-custom-datasets.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "w38kvAPZxyk4",
        "lYhzvGTb1TlR"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOTHuUkHsb5X"
      },
      "source": [
        "# Imports needed\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from google.colab import drive \n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzAFC96nxlSv"
      },
      "source": [
        "img_height = 28\n",
        "img_width = 28\n",
        "batch_size = 2\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.InputLayer((28, 28, 1)),\n",
        "        layers.Conv2D(16, 3, padding=\"same\"),\n",
        "        layers.Conv2D(32, 3, padding=\"same\"),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAwwtyF9yxu-",
        "outputId": "4a8e91b5-7cb9-407c-a605-450686f69b1e"
      },
      "source": [
        "random.seed(42)\n",
        "gdrive_path = '/content/gdrive'\n",
        "drive.mount(gdrive_path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sweKl6J5y0ZY"
      },
      "source": [
        "dataset_path = '/content/gdrive/My Drive/Colab Notebooks/CV/Dataset_Transferlearning/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w38kvAPZxyk4"
      },
      "source": [
        "### Method 1\n",
        "\n",
        "Using dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByLGKBx8xnnA",
        "outputId": "2f19921b-7cea-4323-fb24-70c6cf5dd581"
      },
      "source": [
        "ds_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_path+\"data/mnist_subfolders/\",\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",  # categorical, binary\n",
        "    # class_names=['0', '1', '2', '3', ...]\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=batch_size,\n",
        "    image_size=(img_height, img_width),  # reshape if not in this size\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.1,\n",
        "    subset=\"training\",\n",
        ")\n",
        "\n",
        "ds_validation = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_path+\"data/mnist_subfolders/\",\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",  # categorical, binary\n",
        "    # class_names=['0', '1', '2', '3', ...]\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=batch_size,\n",
        "    image_size=(img_height, img_width),  # reshape if not in this size\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.1,\n",
        "    subset=\"validation\",\n",
        ")\n",
        "\n",
        "\n",
        "def augment(x, y):\n",
        "    image = tf.image.random_brightness(x, max_delta=0.05)\n",
        "    return image, y\n",
        "\n",
        "\n",
        "ds_train = ds_train.map(augment)\n",
        "\n",
        "# Custom Loops\n",
        "for epochs in range(10):\n",
        "    for x, y in ds_train:\n",
        "        # train here\n",
        "        pass\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=[keras.losses.SparseCategoricalCrossentropy(from_logits=True),],\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(ds_train, epochs=10, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 50 files belonging to 10 classes.\n",
            "Using 45 files for training.\n",
            "Found 50 files belonging to 10 classes.\n",
            "Using 5 files for validation.\n",
            "Epoch 1/10\n",
            "23/23 - 1s - loss: 66.3582 - accuracy: 0.2222\n",
            "Epoch 2/10\n",
            "23/23 - 0s - loss: 10.7957 - accuracy: 0.7333\n",
            "Epoch 3/10\n",
            "23/23 - 0s - loss: 2.3985 - accuracy: 0.8889\n",
            "Epoch 4/10\n",
            "23/23 - 0s - loss: 0.3245 - accuracy: 0.9333\n",
            "Epoch 5/10\n",
            "23/23 - 0s - loss: 0.0195 - accuracy: 0.9778\n",
            "Epoch 6/10\n",
            "23/23 - 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "23/23 - 0s - loss: 2.9259e-04 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "23/23 - 0s - loss: 1.7151e-05 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "23/23 - 0s - loss: 1.1876e-05 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "23/23 - 0s - loss: 1.0330e-05 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f949f833e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYhzvGTb1TlR"
      },
      "source": [
        "### Method 2\n",
        "\n",
        "ImageDataGenerator and flow_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffk10yzax6LA",
        "outputId": "cdacb047-eaf2-4373-9b3c-3d8f91dbb00a"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=5,\n",
        "    zoom_range=(0.95, 0.95),\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    data_format=\"channels_last\",\n",
        "    validation_split=0.0,\n",
        "    dtype=tf.float32,\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_path+\"data/mnist_subfolders/\",\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=\"grayscale\",\n",
        "    class_mode=\"sparse\",\n",
        "    shuffle=True,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        ")\n",
        "\n",
        "\n",
        "def training():\n",
        "    pass\n",
        "\n",
        "\n",
        "# Custom Loops\n",
        "for epoch in range(10):\n",
        "    num_batches = 0\n",
        "\n",
        "    for x, y in ds_train:\n",
        "        num_batches += 1\n",
        "\n",
        "        # do training\n",
        "        training()\n",
        "\n",
        "        if num_batches == 25:  # len(train_dataset)/batch_size\n",
        "            break\n",
        "\n",
        "# Redo model.compile to reset the optimizer states\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=[keras.losses.SparseCategoricalCrossentropy(from_logits=True),],\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# using model.fit (note steps_per_epoch)\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=25,\n",
        "    verbose=2,\n",
        "    # if we had a validation generator:\n",
        "    # validation_data=validation_generator,\n",
        "    # valiation_steps=len(validation_set)/batch_size),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 50 images belonging to 10 classes.\n",
            "Epoch 1/10\n",
            "25/25 - 1s - loss: 1.6938 - accuracy: 0.5400\n",
            "Epoch 2/10\n",
            "25/25 - 0s - loss: 0.5089 - accuracy: 0.9200\n",
            "Epoch 3/10\n",
            "25/25 - 0s - loss: 0.0990 - accuracy: 0.9800\n",
            "Epoch 4/10\n",
            "25/25 - 0s - loss: 0.0247 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "25/25 - 0s - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "25/25 - 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "25/25 - 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "25/25 - 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "25/25 - 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "25/25 - 0s - loss: 0.0028 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f949ee96710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvZgyePO2Fsx"
      },
      "source": [
        "### Method 3\n",
        "\n",
        "From CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6JM-z4u1aoZ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "directory = dataset_path + \"data/mnist_images_csv/\"\n",
        "df = pd.read_csv(directory + \"train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sUibEjhd2kDF",
        "outputId": "11cd4bdc-3085-405d-efb5-70ce8c56ed5d"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0_1.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_2.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0_3.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0_4.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0_5.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  file_name  label\n",
              "0   0_1.jpg      0\n",
              "1   0_2.jpg      0\n",
              "2   0_3.jpg      0\n",
              "3   0_4.jpg      0\n",
              "4   0_5.jpg      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14WqBsT62a1x",
        "outputId": "44a64902-33c8-404f-e558-de3256837952"
      },
      "source": [
        "file_paths = df[\"file_name\"].values\n",
        "labels = df[\"label\"].values\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "\n",
        "\n",
        "def read_image(image_file, label):\n",
        "    image = tf.io.read_file(directory + image_file)\n",
        "    image = tf.image.decode_image(image, channels=1, dtype=tf.float32)\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def augment(image, label):\n",
        "    # data augmentation here\n",
        "    return image, label\n",
        "\n",
        "\n",
        "ds_train = ds_train.map(read_image).map(augment).batch(2)\n",
        "\n",
        "for epoch in range(10):\n",
        "    for x, y in ds_train:\n",
        "        # train here\n",
        "        pass\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.InputLayer((28, 28, 1)),\n",
        "        layers.Conv2D(16, 3, padding=\"same\"),\n",
        "        layers.Conv2D(32, 3, padding=\"same\"),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=[keras.losses.SparseCategoricalCrossentropy(from_logits=True),],\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(ds_train, epochs=10, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/10\n",
            "25/25 - 0s - loss: 2.7496 - accuracy: 0.0600\n",
            "Epoch 2/10\n",
            "25/25 - 0s - loss: 2.5375 - accuracy: 0.1600\n",
            "Epoch 3/10\n",
            "25/25 - 0s - loss: 1.1304 - accuracy: 0.9400\n",
            "Epoch 4/10\n",
            "25/25 - 0s - loss: 0.4346 - accuracy: 0.9600\n",
            "Epoch 5/10\n",
            "25/25 - 0s - loss: 0.1185 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "25/25 - 0s - loss: 0.0436 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "25/25 - 0s - loss: 0.0239 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "25/25 - 0s - loss: 0.0159 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "25/25 - 0s - loss: 0.0116 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "25/25 - 0s - loss: 0.0089 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f949e359390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa-SNAh92971"
      },
      "source": [
        "### Method 4\n",
        "\n",
        "3 single folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1ZQMhQH4CxV"
      },
      "source": [
        "import pathlib"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTFahYub22ti"
      },
      "source": [
        "batch_size = 2\n",
        "img_height = 28\n",
        "img_width = 28"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Yb6vl34EZi"
      },
      "source": [
        "directory = dataset_path + \"data/mnist_images_only/\"\n",
        "ds_train = tf.data.Dataset.list_files(str(pathlib.Path(directory + \"*.jpg\")))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch5jTtFx9oiO",
        "outputId": "fd7c2f44-d84b-4c54-9049-8c35c1977353"
      },
      "source": [
        "for filepath in ds_train:\n",
        "  print(tf.strings.split(filepath, os.path.sep))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'7_2.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'9_3.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'7_1.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'3_4.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'2_1.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'2_5.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'8_3.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'6_1.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'6_2.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'1_5.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'5_3.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'7_3.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'4_2.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'8_1.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'9_4.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'5_5.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'3_5.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'4_5.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'0_4.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'8_4.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'8_2.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'1_3.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'7_5.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'0_1.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'0_5.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'0_3.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'3_2.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'2_2.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'5_1.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'3_1.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'5_4.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'4_3.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'1_4.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'1_1.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'6_4.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'1_2.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'9_5.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'2_3.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'4_4.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'5_2.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'2_4.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'6_5.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'7_4.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'9_2.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'0_2.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'8_5.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'6_3.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'3_3.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'4_1.jpg'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'' b'content' b'gdrive' b'My Drive' b'Colab Notebooks' b'CV'\n",
            " b'Dataset_Transferlearning' b'data' b'mnist_images_only' b'9_1.jpg'], shape=(10,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unM21uKO5HXr"
      },
      "source": [
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  label = parts[-1] \n",
        "  label = tf.strings.substr(label, pos=0, len=1)\n",
        "  label = tf.strings.to_number(label, out_type=tf.int64)\n",
        "  # Integer encode the label\n",
        "  return label\n",
        "\n",
        "\n",
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=1)\n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [img_height, img_width])\n",
        "\n",
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPpFFduS607Z"
      },
      "source": [
        "ds_train1 = ds_train.map(process_path).batch(batch_size)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hBBKIUw5K2z",
        "outputId": "4af49e37-9016-470f-dd7b-b476f44d6b57"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.InputLayer((28, 28, 1)),\n",
        "        layers.Conv2D(16, 3, padding=\"same\"),\n",
        "        layers.Conv2D(32, 3, padding=\"same\"),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=[keras.losses.SparseCategoricalCrossentropy(from_logits=True),],\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(ds_train1, epochs=10, verbose=2)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "25/25 - 2s - loss: 94.9544 - accuracy: 0.1200\n",
            "Epoch 2/10\n",
            "25/25 - 0s - loss: 5.8504 - accuracy: 0.7000\n",
            "Epoch 3/10\n",
            "25/25 - 0s - loss: 0.0799 - accuracy: 0.9800\n",
            "Epoch 4/10\n",
            "25/25 - 0s - loss: 0.0167 - accuracy: 0.9800\n",
            "Epoch 5/10\n",
            "25/25 - 0s - loss: 2.9078e-05 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "25/25 - 0s - loss: 8.1100e-06 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "25/25 - 0s - loss: 7.5165e-06 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "25/25 - 0s - loss: 6.9254e-06 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "25/25 - 0s - loss: 6.4081e-06 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "25/25 - 0s - loss: 5.9171e-06 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb93e62bb90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKlqkgMb7Acr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "745b0aef-68bd-48cf-9efe-3bc716d2ad83"
      },
      "source": [
        "model.save(dataset_path+'pretrained')\n"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/CV/Dataset_Transferlearning/pretrained/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-wDtzxBZx-y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}